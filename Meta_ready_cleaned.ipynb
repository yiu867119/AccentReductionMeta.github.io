{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d9d6167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: CLEAN & STANDARDIZE RAW DATA\n",
      "================================================================================\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "1.1 Data Import and Initial Standardization\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Moderator variables loaded: 29 rows × 24 columns\n",
      "✓ Effect-size data loaded:    29 rows × 32 columns\n",
      "✓ Whitespace normalized across all string fields\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "1.2 Harmonization of Indexing and Sample Inclusion\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Indices in moderator file:     29\n",
      "  Indices in effect-size file:   29\n",
      "  Common indices (retained):     29\n",
      "\n",
      "✓ Sample harmonization complete:\n",
      "  - Moderator dataset:    29 effect sizes\n",
      "  - Effect-size dataset:  29 effect sizes\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "1.3 Normalization of Missing and Irregular Values\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  [1.3.1] Standardizing missing value representations...\n",
      "  ✓ 11 missing value codes normalized to NaN\n",
      "  [1.3.2] Standardizing Yes/No categorical values...\n",
      "  ✓ Yes/No values standardized (yes/no/y/n → Yes/No)\n",
      "  [1.3.3] Standardizing Age_Group categories...\n",
      "  ✓ Age_Group: adult → Adult, adolescent → Adolescent\n",
      "  [1.3.4] Standardizing Proficiency_Level categories...\n",
      "  ✓ Proficiency_Level: intermediate → Intermediate\n",
      "  ✓ Mixed/compound levels set to NaN\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "1.4 Extraction and Structuring of Effect-Size Measures\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  [1.4.1] Converting gender ratio to proportion of females...\n",
      "  ✓ Gender_Ratio_FM → proportion of females (0-1 scale, 2 decimals)\n",
      "  [1.4.2] Processing training duration variables...\n",
      "  ✓ Training_TotalMinute: expressions evaluated (e.g., '13*90' → 1170.0)\n",
      "  ✓ Training_TotalWeeks: converted to numeric\n",
      "  [1.4.3] Creating Treatment_Duration categorical variable...\n",
      "  ✓ Treatment_Duration created:\n",
      "    - Short (1-4 weeks):   15 studies\n",
      "    - Medium (5-8 weeks):   6 studies\n",
      "    - Long (≥9 weeks):      7 studies\n",
      "    - Missing data:         1 studies\n",
      "\n",
      "✓ STEP 1 COMPLETE: Raw data cleaned and standardized\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 2: MERGE & VALIDATE FINAL DATASET\n",
      "================================================================================\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "2.1 Merge Moderator and Effect-Size Data\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  Effect-size data prepared: 29 rows × 8 columns\n",
      "✓ Datasets merged successfully:\n",
      "  - Final dimensions: 29 rows × 32 columns\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "2.2 Verify Study_ID and Effect_ID Consistency\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  ⚠️  Study_ID mismatches detected: 16 cases\n",
      "    index                     Study_ID        Study_ID_effect\n",
      "1      22       Almalki_Algethami_2022           Almalki_2022\n",
      "8      60            Yeldham_Choy_2022       YeldhamChoy_2022\n",
      "12   85_1            Hirschi_Kang_2025           Hirschi_2025\n",
      "13   85_2            Hirschi_Kang_2025           Hirschi_2025\n",
      "14     86                   Mahdi_2025  Hajmalek_Sabouri_2025\n",
      "16    125      Gooch_Saito_Lyster_2016             Gooch_2016\n",
      "17    127             Wu_Liu_Chen_2021                Wu_2021\n",
      "18    129               Liu_Wu_Ye_2020               Liu_2020\n",
      "19    150          Li_Mohamad_You_2025                Li_2025\n",
      "20    196        Zhang_Chang_Liao_2021              Zhang2021\n",
      "21  216_1     Zhang_Baills_Prieto_2023             Zhang_2023\n",
      "22  216_2     Zhang_Baills_Prieto_2023             Zhang_2023\n",
      "24  224_1                  Dai_Wu_2023               Dai_2023\n",
      "25  224_2                  Dai_Wu_2023               Dai_2023\n",
      "26  224_3                  Dai_Wu_2023               Dai_2023\n",
      "27    226  Huang_Barrett_Lo_Tseng_2024             Huang_2024\n",
      "  ⚠️  Effect_ID mismatches detected: 15 cases\n",
      "    index                              Effect_ID  \\\n",
      "6      38                          Leis2025_Post   \n",
      "8      60                       Yeldham2022_Post   \n",
      "9    61_1        Tseng2025_Nonverbal_Post_Stress   \n",
      "10   61_2     Tseng2025_Nonverbal_Post_Segmental   \n",
      "12   85_1       Hirschi2025_VN_Post_Accentedness   \n",
      "13   85_2  Hirschi2025_VN_Post_Comprehensibility   \n",
      "14     86                         Mahdi2025_Post   \n",
      "16    125                         Gooch2016_Post   \n",
      "19    150                            Li2025_Post   \n",
      "20    196              Zhang2021_Intonation_Post   \n",
      "21  216_1            Zhang2023_Post_Accentedness   \n",
      "22  216_2       Zhang2023_Post_Comprehensibility   \n",
      "24  224_1                         DaiWu2023_Post   \n",
      "25  224_2                         DaiWu2023_Post   \n",
      "26  224_3                         DaiWu2023_Post   \n",
      "\n",
      "                    Effect_ID_effect  \n",
      "6                   Leis2025_Overall  \n",
      "8               YeldhamChoy2022_Post  \n",
      "9              Tseng2025_Post_Stress  \n",
      "10         Tseng2025_Post_Segmentals  \n",
      "12              Hirschi2025_Acc_Post  \n",
      "13             Hirschi2025_Comp_Post  \n",
      "14                 Hajmalek2025_Post  \n",
      "16             Gooch2016_Prompt_Post  \n",
      "19                   Li2025_SAQ_Post  \n",
      "20                 Zhang2021_IT_Post  \n",
      "21            Zhang2023_Accentedness  \n",
      "22  Zhang2023_Comprehensibility_Post  \n",
      "24    Dai2023_Comprehensibility_Post  \n",
      "25            Dai2023_Segmental_Post  \n",
      "26               Dai2023_Stress_Post  \n",
      "  ✓ Duplicate ID columns removed\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "2.3 Export Final Dataset\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Final dataset exported: Meta_ready_cleaned.csv\n",
      "  - Total effect sizes: 29\n",
      "  - Total variables:    30\n",
      "\n",
      "Key transformations applied:\n",
      "  • Gender_Ratio_FM → Proportion of females (0-1 scale, 2 decimals)\n",
      "  • Training_TotalMinute → Numeric minutes (expressions evaluated)\n",
      "  • Treatment_Duration → Categorical (Short/Medium/Long)\n",
      "  • Missing values → Standardized to NaN\n",
      "  • Yes/No values → Standardized capitalization\n",
      "  • Only studies with complete data in both files included\n",
      "\n",
      "================================================================================\n",
      "✅ DATA PREPARATION PIPELINE COMPLETE\n",
      "================================================================================\n",
      "Dataset is now ready for meta-analytic modeling.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "Meta-Analysis Data Preparation Pipeline\n",
    "================================================================================\n",
    "\n",
    "Workflow Overview:\n",
    "-----------------\n",
    "STEP 1: Clean & Standardize Raw Data\n",
    "    ├── 1.1 Data Import and Initial Standardization\n",
    "    ├── 1.2 Harmonization of Indexing and Sample Inclusion\n",
    "    ├── 1.3 Normalization of Missing and Irregular Values\n",
    "    └── 1.4 Extraction and Structuring of Effect-Size Measures\n",
    "\n",
    "STEP 2: Merge & Validate Final Dataset\n",
    "    ├── 2.1 Merge moderator and effect-size data\n",
    "    ├── 2.2 Verify Study_ID and Effect_ID consistency\n",
    "    └── 2.3 Export final dataset\n",
    "\n",
    "Input Files:\n",
    "-----------\n",
    "- moderator_raw.csv: Study-level moderator variables (N studies × K moderators)\n",
    "- effect_size_raw_results.csv: Effect-size estimates (Hedges' g, SE, 95% CI)\n",
    "\n",
    "Output File:\n",
    "-----------\n",
    "- Meta_ready_cleaned.csv: Final merged, cleaned, analysis-ready dataset\n",
    "\n",
    "Dependencies:\n",
    "------------\n",
    "- numpy (>=1.20.0)\n",
    "- pandas (>=1.3.0)\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress pandas warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# STEP 1: CLEAN & STANDARDIZE RAW DATA\n",
    "################################################################################\n",
    "\"\"\"\n",
    "This step implements comprehensive data cleaning and standardization procedures\n",
    "to ensure data quality, consistency, and compatibility for meta-analytic synthesis.\n",
    "\n",
    "Sub-steps:\n",
    "    1.1 Data Import and Initial Standardization\n",
    "    1.2 Harmonization of Indexing and Sample Inclusion  \n",
    "    1.3 Normalization of Missing and Irregular Values\n",
    "    1.4 Extraction and Structuring of Effect-Size Measures\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 1.1 Data Import and Initial Standardization\n",
    "# ==============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: CLEAN & STANDARDIZE RAW DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\" + \"─\" * 80)\n",
    "print(\"1.1 Data Import and Initial Standardization\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "# Load raw data files with explicit type specification\n",
    "# - dtype=str: Preserves original formatting and prevents automatic type inference\n",
    "# - encoding=\"utf-8-sig\": Handles Byte Order Mark (BOM) from Excel-exported CSVs\n",
    "moderator_raw = pd.read_csv(\"moderator_raw.csv\", dtype=str, encoding=\"utf-8-sig\")\n",
    "effect_size_raw = pd.read_csv(\"effect_size_raw_results.csv\", dtype=str, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✓ Moderator variables loaded: {moderator_raw.shape[0]} rows × {moderator_raw.shape[1]} columns\")\n",
    "print(f\"✓ Effect-size data loaded:    {effect_size_raw.shape[0]} rows × {effect_size_raw.shape[1]} columns\")\n",
    "\n",
    "# Trim leading/trailing whitespace from all string cells\n",
    "# Critical for preventing merge failures due to inconsistent formatting\n",
    "moderator_raw = moderator_raw.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "effect_size_raw = effect_size_raw.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "print(\"✓ Whitespace normalized across all string fields\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1.2 Harmonization of Indexing and Sample Inclusion\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"─\" * 80)\n",
    "print(\"1.2 Harmonization of Indexing and Sample Inclusion\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "# Standardize the 'index' column (unique identifier for each effect size)\n",
    "# Ensures consistent string format and removes any extraneous whitespace\n",
    "moderator_raw[\"index\"] = moderator_raw[\"index\"].astype(str).str.strip()\n",
    "effect_size_raw[\"index\"] = effect_size_raw[\"index\"].astype(str).str.strip()\n",
    "\n",
    "# Identify common indices across both datasets\n",
    "# Only effect sizes with complete moderator AND effect-size data are retained\n",
    "# This ensures sample inclusion criteria are consistently applied\n",
    "indices_moderator = set(moderator_raw[\"index\"])\n",
    "indices_effect = set(effect_size_raw[\"index\"])\n",
    "common_indices = indices_moderator & indices_effect  # Set intersection\n",
    "\n",
    "print(f\"  Indices in moderator file:    {len(indices_moderator):3d}\")\n",
    "print(f\"  Indices in effect-size file:  {len(indices_effect):3d}\")\n",
    "print(f\"  Common indices (retained):    {len(common_indices):3d}\")\n",
    "\n",
    "# Identify and report indices unique to each file\n",
    "# These represent incomplete data and will be excluded\n",
    "only_moderator = sorted(indices_moderator - indices_effect)\n",
    "only_effect = sorted(indices_effect - indices_moderator)\n",
    "\n",
    "if only_moderator:\n",
    "    print(f\"\\n  ⚠️  Excluded (moderator only): {only_moderator}\")\n",
    "if only_effect:\n",
    "    print(f\"  ⚠️  Excluded (effect-size only): {only_effect}\")\n",
    "\n",
    "# Filter both datasets to include only common indices\n",
    "# Ensures one-to-one correspondence for subsequent merge\n",
    "moderator_raw = moderator_raw[moderator_raw[\"index\"].isin(common_indices)].copy()\n",
    "effect_size_raw = effect_size_raw[effect_size_raw[\"index\"].isin(common_indices)].copy()\n",
    "\n",
    "print(f\"\\n✓ Sample harmonization complete:\")\n",
    "print(f\"  - Moderator dataset:   {moderator_raw.shape[0]:3d} effect sizes\")\n",
    "print(f\"  - Effect-size dataset: {effect_size_raw.shape[0]:3d} effect sizes\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1.3 Normalization of Missing and Irregular Values\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"─\" * 80)\n",
    "print(\"1.3 Normalization of Missing and Irregular Values\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "# --- 1.3.1 Standardize Missing Value Codes ---\n",
    "print(\"  [1.3.1] Standardizing missing value representations...\")\n",
    "\n",
    "# Define all representations of missing data used in the raw files\n",
    "# These will be standardized to numpy NaN for consistent handling\n",
    "missing_codes = [\n",
    "    \"NG\",      # Not Given\n",
    "    \"N/R\",     # Not Reported\n",
    "    \"n/a\",     # Not Applicable (lowercase)\n",
    "    \"N/A\",     # Not Applicable (uppercase)\n",
    "    \"NA\",      # Not Available\n",
    "    \"NaN\",     # Already NaN as string\n",
    "    \"\",        # Empty string\n",
    "    \"-\",       # Dash placeholder\n",
    "    \"Mixed\",   # Mixed/heterogeneous categories\n",
    "    \"mixed\",   # Mixed (lowercase)\n",
    "    \"MIXED\"    # Mixed (uppercase)\n",
    "]\n",
    "\n",
    "# Replace all missing codes with numpy NaN across both dataframes\n",
    "moderator_raw = moderator_raw.replace(missing_codes, np.nan)\n",
    "effect_size_raw = effect_size_raw.replace(missing_codes, np.nan)\n",
    "print(f\"  ✓ {len(missing_codes)} missing value codes normalized to NaN\")\n",
    "\n",
    "\n",
    "# --- 1.3.2 Standardize Yes/No Values ---\n",
    "print(\"  [1.3.2] Standardizing Yes/No categorical values...\")\n",
    "\n",
    "# Map all variations of Yes/No to standard capitalized format\n",
    "# Ensures consistency for categorical moderator analysis\n",
    "yes_no_map = {\n",
    "    # Yes variations\n",
    "    \"yes\": \"Yes\", \"YES\": \"Yes\", \"y\": \"Yes\", \"Y\": \"Yes\",\n",
    "    # No variations\n",
    "    \"no\": \"No\", \"NO\": \"No\", \"n\": \"No\", \"N\": \"No\"\n",
    "}\n",
    "\n",
    "moderator_raw = moderator_raw.replace(yes_no_map)\n",
    "effect_size_raw = effect_size_raw.replace(yes_no_map)\n",
    "print(\"  ✓ Yes/No values standardized (yes/no/y/n → Yes/No)\")\n",
    "\n",
    "\n",
    "# --- 1.3.3 Standardize Age Group Categories ---\n",
    "print(\"  [1.3.3] Standardizing Age_Group categories...\")\n",
    "\n",
    "if \"Age_Group\" in moderator_raw.columns:\n",
    "    age_group_map = {\n",
    "        \"adult\": \"Adult\",\n",
    "        \"adolescent\": \"Adolescent\",\n",
    "        \"adolescent \": \"Adolescent\"  # Handle trailing space\n",
    "    }\n",
    "    moderator_raw[\"Age_Group\"] = moderator_raw[\"Age_Group\"].replace(age_group_map)\n",
    "    print(\"  ✓ Age_Group: adult → Adult, adolescent → Adolescent\")\n",
    "\n",
    "\n",
    "# --- 1.3.4 Standardize Proficiency Level Categories ---\n",
    "print(\"  [1.3.4] Standardizing Proficiency_Level categories...\")\n",
    "\n",
    "if \"Proficiency_Level\" in moderator_raw.columns:\n",
    "    # Convert to consistent capitalization\n",
    "    # Mixed/compound levels (e.g., \"Intermediate_Advanced\") → NaN\n",
    "    # because they cannot be analyzed as a single proficiency category\n",
    "    proficiency_map = {\n",
    "        \"intermediate\": \"Intermediate\",\n",
    "        \"Intermediate_Advanced\": np.nan,  # Compound level → missing\n",
    "        \"mixed\": np.nan,\n",
    "        \"Mixed\": np.nan\n",
    "    }\n",
    "    moderator_raw[\"Proficiency_Level\"] = moderator_raw[\"Proficiency_Level\"].replace(proficiency_map)\n",
    "    print(\"  ✓ Proficiency_Level: intermediate → Intermediate\")\n",
    "    print(\"  ✓ Mixed/compound levels set to NaN\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1.4 Extraction and Structuring of Effect-Size Measures\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"─\" * 80)\n",
    "print(\"1.4 Extraction and Structuring of Effect-Size Measures\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "# --- 1.4.1 Transform Gender Ratio to Proportion ---\n",
    "print(\"  [1.4.1] Converting gender ratio to proportion of females...\")\n",
    "\n",
    "if \"Gender_Ratio_FM\" in moderator_raw.columns:\n",
    "    # Store original column position to maintain column order\n",
    "    gender_col_position = moderator_raw.columns.get_loc(\"Gender_Ratio_FM\")\n",
    "    \n",
    "    # Extract female and male counts from \"numberF/numberM\" format (e.g., \"20F/12M\")\n",
    "    # Regex: ^\\s*(\\d+)\\s*[Ff]\\s*/\\s*(\\d+)\\s*[Mm]\\s*$\n",
    "    #   - Captures: (female_count) F / (male_count) M\n",
    "    gender_parts = moderator_raw[\"Gender_Ratio_FM\"].str.extract(\n",
    "        r\"^\\s*(\\d+)\\s*[Ff]\\s*/\\s*(\\d+)\\s*[Mm]\\s*$\"\n",
    "    )\n",
    "    \n",
    "    # Convert to numeric and calculate proportion\n",
    "    n_female = pd.to_numeric(gender_parts[0], errors=\"coerce\")\n",
    "    n_male = pd.to_numeric(gender_parts[1], errors=\"coerce\")\n",
    "    n_total = n_female + n_male\n",
    "    proportion_female = (n_female / n_total).round(2)\n",
    "    \n",
    "    # Replace original column with proportion values\n",
    "    moderator_raw[\"Gender_Ratio_FM\"] = proportion_female\n",
    "    \n",
    "    # Restore column to original position\n",
    "    cols = moderator_raw.columns.tolist()\n",
    "    if cols[gender_col_position] != \"Gender_Ratio_FM\":\n",
    "        cols.remove(\"Gender_Ratio_FM\")\n",
    "        cols.insert(gender_col_position, \"Gender_Ratio_FM\")\n",
    "        moderator_raw = moderator_raw[cols]\n",
    "    \n",
    "    print(f\"  ✓ Gender_Ratio_FM → proportion of females (0-1 scale, 2 decimals)\")\n",
    "\n",
    "\n",
    "# --- 1.4.2 Transform Training Duration Variables ---\n",
    "print(\"  [1.4.2] Processing training duration variables...\")\n",
    "\n",
    "# Convert Training_TotalMinute (handles arithmetic expressions)\n",
    "if \"Training_TotalMinute\" in moderator_raw.columns:\n",
    "    minutes_str = moderator_raw[\"Training_TotalMinute\"].astype(str).str.strip()\n",
    "    minutes_str = minutes_str.str.replace(\" \", \"\", regex=False)  # Remove internal spaces\n",
    "    \n",
    "    minutes_numeric = pd.Series([np.nan] * len(minutes_str), index=moderator_raw.index)\n",
    "    \n",
    "    for idx, val in minutes_str.items():\n",
    "        if pd.isna(val) or val == \"nan\" or val == \"\":\n",
    "            minutes_numeric[idx] = np.nan\n",
    "        elif \"*\" in val:\n",
    "            # Multiplication expression (e.g., \"13*90\" = weeks × minutes_per_week)\n",
    "            try:\n",
    "                parts = val.split(\"*\")\n",
    "                if len(parts) == 2:\n",
    "                    minutes_numeric[idx] = round(float(parts[0]) * float(parts[1]), 2)\n",
    "                else:\n",
    "                    minutes_numeric[idx] = np.nan\n",
    "            except:\n",
    "                minutes_numeric[idx] = np.nan\n",
    "        else:\n",
    "            # Direct numeric value\n",
    "            try:\n",
    "                minutes_numeric[idx] = round(float(val), 2)\n",
    "            except:\n",
    "                minutes_numeric[idx] = np.nan\n",
    "    \n",
    "    moderator_raw[\"Training_TotalMinute\"] = minutes_numeric\n",
    "    print(\"  ✓ Training_TotalMinute: expressions evaluated (e.g., '13*90' → 1170.0)\")\n",
    "\n",
    "# Convert Training_TotalWeeks to numeric\n",
    "if \"Training_TotalWeeks\" in moderator_raw.columns:\n",
    "    moderator_raw[\"Training_TotalWeeks\"] = pd.to_numeric(\n",
    "        moderator_raw[\"Training_TotalWeeks\"], errors=\"coerce\"\n",
    "    )\n",
    "    print(\"  ✓ Training_TotalWeeks: converted to numeric\")\n",
    "\n",
    "\n",
    "# --- 1.4.3 Create Treatment Duration Categorical Variable ---\n",
    "print(\"  [1.4.3] Creating Treatment_Duration categorical variable...\")\n",
    "\n",
    "if \"Training_TotalWeeks\" in moderator_raw.columns:\n",
    "    \n",
    "    def categorize_duration(weeks):\n",
    "        \"\"\"\n",
    "        Categorize training duration into Short/Medium/Long based on week count.\n",
    "        \n",
    "        Classification scheme:\n",
    "            - Short:  1-4 weeks\n",
    "            - Medium: 5-8 weeks  \n",
    "            - Long:   ≥9 weeks\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        weeks : float or NaN\n",
    "            Number of training weeks\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str or NaN\n",
    "            Duration category ('Short', 'Medium', or 'Long')\n",
    "        \"\"\"\n",
    "        if pd.isna(weeks):\n",
    "            return np.nan\n",
    "        elif weeks <= 4:\n",
    "            return \"Short\"\n",
    "        elif weeks <= 8:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Long\"\n",
    "    \n",
    "    # Apply categorization\n",
    "    moderator_raw[\"Treatment_Duration\"] = moderator_raw[\"Training_TotalWeeks\"].apply(categorize_duration)\n",
    "    \n",
    "    # Position Treatment_Duration column right after Training_TotalWeeks\n",
    "    weeks_col_position = moderator_raw.columns.get_loc(\"Training_TotalWeeks\")\n",
    "    cols = moderator_raw.columns.tolist()\n",
    "    cols.remove(\"Treatment_Duration\")\n",
    "    cols.insert(weeks_col_position + 1, \"Treatment_Duration\")\n",
    "    moderator_raw = moderator_raw[cols]\n",
    "    \n",
    "    # Report distribution\n",
    "    duration_counts = moderator_raw[\"Treatment_Duration\"].value_counts(dropna=False)\n",
    "    print(f\"  ✓ Treatment_Duration created:\")\n",
    "    print(f\"    - Short (1-4 weeks):   {duration_counts.get('Short', 0):2d} studies\")\n",
    "    print(f\"    - Medium (5-8 weeks):  {duration_counts.get('Medium', 0):2d} studies\")\n",
    "    print(f\"    - Long (≥9 weeks):     {duration_counts.get('Long', 0):2d} studies\")\n",
    "    \n",
    "    missing_count = moderator_raw[\"Treatment_Duration\"].isna().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"    - Missing data:        {missing_count:2d} studies\")\n",
    "\n",
    "\n",
    "print(\"\\n✓ STEP 1 COMPLETE: Raw data cleaned and standardized\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# STEP 2: MERGE & VALIDATE FINAL DATASET\n",
    "################################################################################\n",
    "\"\"\"\n",
    "This step merges the cleaned moderator and effect-size datasets, validates\n",
    "data integrity, and exports the final analysis-ready dataset.\n",
    "\n",
    "Sub-steps:\n",
    "    2.1 Merge moderator and effect-size data\n",
    "    2.2 Verify Study_ID and Effect_ID consistency  \n",
    "    2.3 Export final dataset\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: MERGE & VALIDATE FINAL DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2.1 Merge Moderator and Effect-Size Data\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"─\" * 80)\n",
    "print(\"2.1 Merge Moderator and Effect-Size Data\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "# Prepare effect-size data for merging\n",
    "# Select only essential columns for meta-analysis\n",
    "effect_clean = effect_size_raw[[\n",
    "    \"index\",        # Merge key (unique identifier)\n",
    "    \"Study_ID\",     # Study identifier\n",
    "    \"Effect_ID\",    # Effect-size identifier  \n",
    "    \"Hedges_g\",     # Effect-size estimate (bias-corrected)\n",
    "    \"SE\",           # Standard error\n",
    "    \"Variance\",     # Variance of effect size\n",
    "    \"CI_Lower\",     # 95% CI lower bound\n",
    "    \"CI_Upper\"      # 95% CI upper bound\n",
    "]].copy()\n",
    "\n",
    "# Rename ID columns to enable validation after merge\n",
    "effect_clean = effect_clean.rename(columns={\n",
    "    \"Study_ID\": \"Study_ID_effect\",\n",
    "    \"Effect_ID\": \"Effect_ID_effect\"\n",
    "})\n",
    "\n",
    "print(f\"  Effect-size data prepared: {effect_clean.shape[0]} rows × {effect_clean.shape[1]} columns\")\n",
    "\n",
    "# Perform inner merge on 'index' column\n",
    "# - how=\"inner\": Retain only rows present in BOTH datasets\n",
    "# - validate=\"1:1\": Enforce one-to-one relationship (no duplicate indices)\n",
    "merged_df = pd.merge(\n",
    "    moderator_raw,\n",
    "    effect_clean,\n",
    "    on=\"index\",\n",
    "    how=\"inner\",      # Inner join: only matched indices\n",
    "    validate=\"1:1\"    # Enforce 1:1 relationship\n",
    ")\n",
    "\n",
    "print(f\"✓ Datasets merged successfully:\")\n",
    "print(f\"  - Final dimensions: {merged_df.shape[0]} rows × {merged_df.shape[1]} columns\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2.2 Verify Study_ID and Effect_ID Consistency\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"─\" * 80)\n",
    "print(\"2.2 Verify Study_ID and Effect_ID Consistency\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "# Check Study_ID consistency across datasets\n",
    "study_mismatch = merged_df[\n",
    "    merged_df[\"Study_ID\"].astype(str) != merged_df[\"Study_ID_effect\"].astype(str)\n",
    "]\n",
    "\n",
    "if len(study_mismatch) > 0:\n",
    "    print(f\"  ⚠️  Study_ID mismatches detected: {len(study_mismatch)} cases\")\n",
    "    print(study_mismatch[[\"index\", \"Study_ID\", \"Study_ID_effect\"]])\n",
    "else:\n",
    "    print(\"  ✓ Study_ID validation: All values match between datasets\")\n",
    "\n",
    "# Check Effect_ID consistency across datasets\n",
    "effect_mismatch = merged_df[\n",
    "    merged_df[\"Effect_ID\"].astype(str) != merged_df[\"Effect_ID_effect\"].astype(str)\n",
    "]\n",
    "\n",
    "if len(effect_mismatch) > 0:\n",
    "    print(f\"  ⚠️  Effect_ID mismatches detected: {len(effect_mismatch)} cases\")\n",
    "    print(effect_mismatch[[\"index\", \"Effect_ID\", \"Effect_ID_effect\"]])\n",
    "else:\n",
    "    print(\"  ✓ Effect_ID validation: All values match between datasets\")\n",
    "\n",
    "# Remove duplicate ID columns (keep original columns from moderator dataset)\n",
    "merged_df = merged_df.drop(columns=[\"Study_ID_effect\", \"Effect_ID_effect\"])\n",
    "print(f\"  ✓ Duplicate ID columns removed\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2.3 Export Final Dataset\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"─\" * 80)\n",
    "print(\"2.3 Export Final Dataset\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "# Export final cleaned dataset to CSV\n",
    "# - index=False: Don't write pandas row numbers\n",
    "# - encoding=\"utf-8-sig\": Include BOM for Excel compatibility\n",
    "output_filename = \"Meta_ready_cleaned.csv\"\n",
    "merged_df.to_csv(output_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✓ Final dataset exported: {output_filename}\")\n",
    "print(f\"  - Total effect sizes: {merged_df.shape[0]}\")\n",
    "print(f\"  - Total variables:    {merged_df.shape[1]}\")\n",
    "print(f\"\\nKey transformations applied:\")\n",
    "print(f\"  • Gender_Ratio_FM → Proportion of females (0-1 scale, 2 decimals)\")\n",
    "print(f\"  • Training_TotalMinute → Numeric minutes (expressions evaluated)\")\n",
    "print(f\"  • Treatment_Duration → Categorical (Short/Medium/Long)\")\n",
    "print(f\"  • Missing values → Standardized to NaN\")\n",
    "print(f\"  • Yes/No values → Standardized capitalization\")\n",
    "print(f\"  • Only studies with complete data in both files included\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ DATA PREPARATION PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Dataset is now ready for meta-analytic modeling.\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
